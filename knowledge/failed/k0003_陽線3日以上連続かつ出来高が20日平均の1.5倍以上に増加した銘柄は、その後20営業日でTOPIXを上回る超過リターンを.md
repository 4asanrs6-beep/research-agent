# 陽線3日以上連続かつ出来高が20日平均の1.5倍以上に増加した銘柄は、その後20営業日でTOPIXを上回る超過リターンを獲得できる

- **Knowledge ID**: 3
- **Run ID**: 15
- **判定**: invalid
- **作成日**: 2026-02-23 11:12:37
- **タグ**: モメンタム, 連続陽線, 出来高, イベントスタディ, 日本株, TOPIX, 統計的非有意, 短期売買, テクニカル分析

## 分析条件

- **対象**: 東証上場全銘柄のうち、シグナル発生日時点で時価総額50億円以上の銘柄
- **期間**: 2025-01-01 ~ 2026-02-23
- **アプローチ**: イベントスタディ方式を主軸とし、シグナル発生日をイベント日として定義し、その後1ヶ月（20営業日）のTOPIX対比超過リターンを計測する。全シグナルを集計し、超過リターンの統計的有意性を検定する。
  1. ステップ1: データ取得と前処理 — J-Quants APIから分析期間の全銘柄日足データ・銘柄一覧・TOPIX日次データを取得し、時価総額50億円以上の銘柄にフィルタリングする。
  2. ステップ2: シグナル条件の定義と検出 — 各銘柄・各営業日について①陽線判定（調整後終値>調整後始値）、②連続陽線日数のカウント（3日以上）、③当日出来高÷直近20営業日出来高平均の比率算出（1.5倍以上）を行い、両条件を同時に満たす日をシグナル発生日とする。
  3. ステップ3: フォワードリターンの算出 — 各シグナル発生日の翌営業日始値を基準に、5営業日後・10営業日後・20営業日後の終値までのリターンを算出する。同期間のTOPIXリターンも算出し、超過リターン（銘柄リターン − TOPIXリターン）を計算する。
  4. ステップ4: 重複シグナルの処理 — 同一銘柄で連続してシグナルが発生した場合、最初のシグナルから20営業日以内の重複シグナルは除外し、サンプルの独立性を確保する。
  5. ステップ5: 記述統計と分布分析 — 超過リターンの平均値・中央値・標準偏差・勝率（超過リターン>0の割合）・最大値・最小値を算出する。超過リターンの分布をヒストグラムで可視化する。
  6. ステップ6: 統計的有意性の検定 — 超過リターンの平均がゼロと有意に異なるかをt検定およびWilcoxon符号順位検定で検証する。
  7. ステップ7: サブグループ分析 — 連続陽線日数別（3日・4日・5日以上）、出来高倍率別（1.5〜2.0倍・2.0〜3.0倍・3.0倍以上）、時価総額規模別（50〜300億・300〜1000億・1000億以上）に超過リターンを比較し、条件の強度と効果の関係を検証する。
  8. ステップ8: 累積超過リターンの時系列推移 — シグナル発生日をDay 0として、Day+1〜Day+20の平均累積超過リターン（CAR）を日次でプロットし、リターンの発生タイミングとパターンを把握する。
  9. ステップ9: ロバストネスチェック — 連続陽線日数の閾値（2日・4日・5日）や出来高倍率の閾値（1.2倍・2.0倍・3.0倍）を変化させた感度分析を実施し、結果の頑健性を確認する。

## 統計結果

| 指標 | 値 |
|------|-----|
| p値 | 0.1625 |
| t統計量 | 1.404 |
| Cohen's d | 0.114 |
| 条件群平均 | 114.3100% |
| 基準群平均 | 179.0900% |
| 条件群勝率 | 4868.0% |
| 基準群勝率 | 5000.0% |
| 条件群N | 152 |
| 基準群N | 152 |
| 有意性 | 非有意 |

## バックテスト結果

| 指標 | 値 |
|------|-----|
| 累計リターン | 1817.00% |
| 年率リターン | 1646.00% |
| シャープ比 | 1.59 |
| 最大DD | -1113.00% |
| 勝率 | 4868.0% |
| 取引回数 | 152 |
| BM累計リターン | 3817.00% |

## 解釈

本仮説は統計的に有意ではなく（t検定 p=0.162、Wilcoxon p=0.857）、効果量も極めて小さい（Cohen's d=0.114）。バックテストにおいても累積リターン18.17%はベンチマーク（TOPIX）の38.17%を大幅に下回り、シグナルに基づく戦略が市場平均に劣後することが明確に示された。勝率48.68%・中央値超過リターン-0.24%という結果は、シグナルに体系的な予測力がないことを裏付けている。

**判断理由:**
- t検定のp値0.162およびWilcoxon検定のp値0.857はいずれも有意水準0.05を大幅に上回り、超過リターンの平均がゼロと統計的に有意に異なるとは言えない
- Cohen's d=0.114は「小さい効果」の閾値0.2にも満たず、シグナルの実質的な予測力は無視できる水準である
- バックテスト累積リターン18.17%はベンチマーク38.17%に対して約20ポイントの劣後であり、この戦略を実行するよりTOPIXインデックスを保有した方が遥かに優れた結果を得られた
- 勝率48.68%は50%を下回り、中央値超過リターンが-0.24%と負であることから、平均値はごく一部の外れ値（四国化成HD +63%等）に引き上げられており、典型的なシグナルでは超過リターンを期待できない
- 出来高倍率3.0倍以上のサブグループは平均超過リターン-2.49%・勝率26.67%と顕著に悪化しており、極端な出来高急増は過熱・反転リスクのシグナルとして機能している可能性がある

**強み:**
- イベントスタディ方式の採用、重複シグナル除外による独立性確保など、分析フレームワークの設計は方法論的に適切である
- t検定とWilcoxon検定の併用により、分布の非正規性にも対応したロバストな検定を実施している
- サブグループ分析（連続陽線日数別・出来高倍率別・時価総額別）により、条件の強度と効果の関係を多面的に検証している
- 連続陽線4日のサブグループ（n=23, 平均+3.54%, 勝率65.22%）は限定的ながら興味深い傾向を示しており、追加検証の余地がある
- 5営業日・10営業日・20営業日の段階的リターン計測により、リターン発生の時間構造を把握できている

**弱み:**
- 全体サンプル数152件は統計的検出力の観点からやや不足しており、特にサブグループ分析では5日以上連続陽線（n=25）や出来高3倍以上（n=15）のサンプルが極めて少なく信頼性が低い
- 分析期間が約14ヶ月と短く、相場環境（2025年の上昇相場）に大きく依存した結果である可能性が高い。複数の市場サイクルを跨ぐ長期検証が不足している
- バックテストのシャープ比1.59はベンチマーク1.55と近似しているが、これは資金のフル投資ではなくシグナル発生時のみのポジションであるため、待機期間の機会コストを過小評価している
- recent_examplesの直近5件のリターンがすべて0.0%であるのは20営業日未経過のためと推測されるが、評価可能な直近サンプルの結果が混在しており、最新期間での有効性が不明確である
- 時価総額50億円というフィルタは小型株を多く含み、流動性リスクやスリッページ、マーケットインパクトの影響が考慮されていない

**改善提案:**
- 連続陽線4日のサブグループが有望な傾向を示しているため、陽線4日以上を主条件とし、出来高倍率1.5〜2.0倍に限定した絞り込み条件で再検証すべき（出来高3倍以上は除外が妥当）
- 分析期間を2015年以降等に拡大し、上昇・下落・レンジ相場それぞれにおけるシグナルの有効性を確認する長期バックテストを実施すべき
- セクター別・業種別の分析を追加し、特定のセクター（例：景気敏感株 vs ディフェンシブ株）でシグナルの有効性に差異があるか検証する
- 逆に出来高3倍以上の急増を反転シグナル（ショート候補）として検証する価値がある（平均-2.49%、勝率26.67%は逆張り戦略の候補となりうる）
- スリッページ・売買手数料・マーケットインパクトを考慮した実践的なバックテストを追加し、ネットリターンベースでの評価を行うべき
- 市場全体のモメンタム（TOPIX 20日リターン等）を制御変数として加え、シグナルの効果が市場環境に依存するか交互作用分析を実施する

## 結論

**この仮説は無効と判定されました。**

無効条件: Cohen's d=0.114は「小さい効果」の閾値0.2にも満たず、シグナルの実質的な予測力は無視できる水準である

## 分析コード

```python
import pandas as pd
import numpy as np
from scipy import stats

def run_analysis(data_provider):
    # ============================================================
    # ステップ1: データ取得と前処理
    # ============================================================
    START_DATE = "2025-01-01"
    END_DATE = "2026-02-23"
    WARMUP_START = "2024-11-01"  # 20日MA用ウォームアップ

    # 上場銘柄一覧取得
    stocks_df = data_provider.get_listed_stocks()

    # 時価総額フィルタ用にscale_categoryで大型〜中型を選択
    # scale_categoryがある銘柄 = TOPIX構成銘柄 ≒ 一定時価総額以上
    valid_scales = [
        "TOPIX Core30", "TOPIX Large70", "TOPIX Mid400",
        "TOPIX Small 1", "TOPIX Small 2"
    ]
    filtered_stocks = stocks_df[
        stocks_df["scale_category"].isin(valid_scales)
    ].copy()

    # サンプリング: タイムアウト防止のため最大50銘柄をランダム抽出
    universe_codes = filtered_stocks["code"].tolist()
    np.random.seed(42)
    if len(universe_codes) > 50:
        universe_codes = list(np.random.choice(universe_codes, size=50, replace=False))

    # TOPIX指数データ取得
    topix_df = data_provider.get_index_prices(
        index_code="0000", start_date=WARMUP_START, end_date=END_DATE
    )
    topix_df["date"] = pd.to_datetime(topix_df["date"])
    topix_df = topix_df.sort_values("date").reset_index(drop=True)
    topix_df.rename(columns={"close": "topix_close", "open": "topix_open"}, inplace=True)

    # 各銘柄の株価データ取得
    all_prices = []
    for code in universe_codes:
        try:
            df = data_provider.get_price_daily(
                code=code, start_date=WARMUP_START, end_date=END_DATE
            )
            if df is not None and len(df) > 0:
                all_prices.append(df)
        except Exception:
            continue

    if len(all_prices) == 0:
        return _empty_result(universe_codes, START_DATE, END_DATE)

    prices_df = pd.concat(all_prices, ignore_index=True)
    prices_df["date"] = pd.to_datetime(prices_df["date"])
    prices_df = prices_df.sort_values(["code", "date"]).reset_index(drop=True)

    # 実際に取得できた銘柄コードを更新
    universe_codes = prices_df["code"].unique().tolist()

    # ============================================================
    # ステップ2: シグナル条件の検出
    # ============================================================
    signals_list = []

    for code, grp in prices_df.groupby("code"):
        grp = grp.sort_values("date").reset_index(drop=True)
        if len(grp) < 25:
            continue

        # 調整後価格を使用（なければ生値）
        o = grp["adj_open"].values if "adj_open" in grp.columns else grp["open"].values
        c = grp["adj_close"].values if "adj_close" in grp.columns else grp["close"].values
        v = grp["adj_volume"].values if "adj_volume" in grp.columns else grp["volume"].values
        dates = grp["date"].values

        # 陽線判定
        bullish = (c > o).astype(int)

        # 連続陽線日数カウント
        consec = np.zeros(len(grp), dtype=int)
        for i in range(len(grp)):
            if bullish[i] == 1:
                consec[i] = (consec[i - 1] + 1) if i > 0 else 1
            else:
                consec[i] = 0

        # 出来高20日移動平均
        vol_ma20 = pd.Series(v.astype(float)).rolling(window=20).mean().values
        vol_ratio = np.where(vol_ma20 > 0, v.astype(float) / vol_ma20, 0.0)

        # シグナル条件: 連続陽線3日以上 & 出来高倍率1.5以上
        # 分析期間内のみ
        start_ts = np.datetime64(pd.Timestamp(START_DATE))
        for i in range(len(grp)):
            if dates[i] < start_ts:
                continue
            if consec[i] >= 3 and vol_ratio[i] >= 1.5:
                signals_list.append({
                    "date": dates[i],
                    "code": code,
                    "consec_days": int(consec[i]),
                    "vol_ratio": float(vol_ratio[i]),
                    "close": float(c[i]),
                    "open": float(o[i]),
                })

    if len(signals_list) == 0:
        return _empty_result(universe_codes, START_DATE, END_DATE)

    signals_df = pd.DataFrame(signals_list)
    signals_df["date"] = pd.to_datetime(signals_df["date"])
    signals_df = signals_df.sort_values(["code", "date"]).reset_index(drop=True)

    # ============================================================
    # ステップ4: 重複シグナルの除外（同一銘柄20営業日以内）
    # ============================================================
    filtered_signals = []
    for code, grp in signals_df.groupby("code"):
        grp = grp.sort_values("date")
        last_signal_date = None
        for _, row in grp.iterrows():
            if last_signal_date is None or (row["date"] - last_signal_date).days >= 28:
                filtered_signals.append(row)
                last_signal_date = row["date"]

    if len(filtered_signals) == 0:
        return _empty_result(universe_codes, START_DATE, END_DATE)

    signals_df = pd.DataFrame(filtered_signals).reset_index(drop=True)
    signals_df["date"] = pd.to_datetime(signals_df["date"])

    # ============================================================
    # ステップ3: フォワードリターンの算出
    # ============================================================
    topix_dates = topix_df["date"].values.astype("datetime64[ns]")
    topix_closes = topix_df["topix_close"].values

    def get_topix_return(start_date, n_days):
        sd = np.datetime64(pd.Timestamp(start_date), "ns")
        idx = np.searchsorted(topix_dates, sd)
        if idx >= len(topix_dates):
            return np.nan
        end_idx = idx + n_days
        if end_idx >= len(topix_dates):
            return np.nan
        return float(topix_closes[end_idx] / topix_closes[idx] - 1.0)

    results_list = []
    for _, sig in signals_df.iterrows():
        code = sig["code"]
        sig_date = pd.Timestamp(sig["date"])
        code_prices = prices_df[prices_df["code"] == code].sort_values("date")
        c_col = "adj_close" if "adj_close" in code_prices.columns else "close"
        o_col = "adj_open" if "adj_open" in code_prices.columns else "open"

        dates_arr = code_prices["date"].values.astype("datetime64[ns]")
        close_arr = code_prices[c_col].values
        open_arr = code_prices[o_col].values

        sig_dt64 = np.datetime64(sig_date, "ns")
        sig_idx = np.searchsorted(dates_arr, sig_dt64)
        if sig_idx >= len(dates_arr):
            continue

        # 翌営業日始値をエントリー価格とする
        entry_idx = sig_idx + 1
        if entry_idx >= len(dates_arr):
            continue
        entry_price = float(open_arr[entry_idx])
        entry_date = dates_arr[entry_idx]
        if entry_price <= 0:
            continue

        row = {
            "signal_date": sig_date,
            "entry_date": entry_date,
            "code": code,
            "entry_price": entry_price,
            "consec_days": sig["consec_days"],
            "vol_ratio": sig["vol_ratio"],
        }

        for horizon, label in [(5, "5d"), (10, "10d"), (20, "20d")]:
            h_idx = entry_idx + horizon
            if h_idx < len(dates_arr):
                exit_price = float(close_arr[h_idx])
                stock_ret = exit_price / entry_price - 1.0
                topix_ret = get_topix_return(entry_date, horizon)
                row[f"ret_{label}"] = stock_ret
                row[f"topix_ret_{label}"] = topix_ret if not np.isnan(topix_ret) else 0.0
                row[f"excess_ret_{label}"] = stock_ret - row[f"topix_ret_{label}"]
                if label == "20d":
                    row["exit_price"] = exit_price
                    row["exit_date"] = dates_arr[h_idx]
            else:
                row[f"ret_{label}"] = np.nan
                row[f"topix_ret_{label}"] = np.nan
                row[f"excess_ret_{label}"] = np.nan

        results_list.append(row)

    if len(results_list) == 0:
        return _empty_result(universe_codes, START_DATE, END_DATE)

    results_df = pd.DataFrame(results_list)

    # ============================================================
    # ステップ5&6: 統計分析
    # ============================================================
    er_20d = results_df["excess_ret_20d"].dropna()
    ret_20d = results_df["ret_20d"].dropna()
    topix_ret_20d = results_df["topix_ret_20d"].dropna()

    n_condition = len(er_20d)
    cond_mean = float(er_20d.mean()) if n_condition > 0 else 0.0
    cond_std = float(er_20d.std()) if n_condition > 1 else 0.0
    cond_median = float(er_20d.median()) if n_condition > 0 else 0.0
    base_mean = float(topix_ret_20d.mean()) if len(topix_ret_20d) > 0 else 0.0
    base_std = float(topix_ret_20d.std()) if len(topix_ret_20d) > 1 else 0.0
    win_rate_cond = float((er_20d > 0).mean()) if n_condition > 0 else 0.0

    # t検定 (超過リターンの平均 ≠ 0)
    if n_condition >= 2:
        t_stat, p_val = stats.ttest_1samp(er_20d.values, 0)
        t_stat = float(t_stat)
        p_val = float(p_val)
    else:
        t_stat, p_val = 0.0, 1.0

    cohens_d = cond_mean / cond_std if cond_std > 0 else 0.0

    # Wilcoxon検定
    if n_condition >= 10:
        try:
            w_stat, w_pval = stats.wilcoxon(er_20d.values)
        except Exception:
            w_stat, w_pval = 0.0, 1.0
    else:
        w_stat, w_pval = 0.0, 1.0

    # 5d, 10d excess return stats
    er_5d = results_df["excess_ret_5d"].dropna()
    er_10d = results_df["excess_ret_10d"].dropna()

    statistics = {
        "test_name": "連続陽線・出来高急増シグナルの超過リターン検証（イベントスタディ）",
        "condition_mean": round(cond_mean * 100, 4),
        "baseline_mean": round(base_mean * 100, 4),
        "condition_std": round(cond_std * 100, 4),
        "baseline_std": round(base_std * 100, 4),
        "t_statistic": round(t_stat, 4),
        "p_value": round(p_val, 6),
        "cohens_d": round(cohens_d, 4),
        "win_rate_condition": round(win_rate_cond * 100, 2),
        "win_rate_baseline": 50.0,
        "n_condition": n_condition,
        "n_baseline": n_condition,
        "is_significant": p_val < 0.05,
        "median_excess_return_pct": round(cond_median * 100, 4),
        "wilcoxon_stat": round(float(w_stat), 4),
        "wilcoxon_p_value": round(float(w_pval), 6),
        "excess_ret_5d_mean_pct": round(float(er_5d.mean()) * 100, 4) if len(er_5d) > 0 else 0.0,
        "excess_ret_10d_mean_pct": round(float(er_10d.mean()) * 100, 4) if len(er_10d) > 0 else 0.0,
        "excess_ret_20d_mean_pct": round(cond_mean * 100, 4),
        "profit_factor": _calc_profit_factor(er_20d),
        "sharpe_like_ratio": round(cond_mean / cond_std, 4) if cond_std > 0 else 0.0,
    }

    # ============================================================
    # ステップ7: サブグループ分析（statisticsに追加）
    # ============================================================
    sub_results = {}
    # 連続陽線日数別
    for label, lo, hi in [("3日", 3, 3), ("4日", 4, 4), ("5日以上", 5, 999)]:
        mask = (results_df["consec_days"] >= lo) & (results_df["consec_days"] <= hi)
        sub_er = results_df.loc[mask, "excess_ret_20d"].dropna()
        sub_results[f"consec_{label}_n"] = len(sub_er)
        sub_results[f"consec_{label}_mean_pct"] = round(float(sub_er.mean()) * 100, 4) if len(sub_er) > 0 else 0.0
        sub_results[f"consec_{label}_winrate"] = round(float((sub_er > 0).mean()) * 100, 2) if len(sub_er) > 0 else 0.0

    # 出来高倍率別
    for label, lo, hi in [("1.5-2.0x", 1.5, 2.0), ("2.0-3.0x", 2.0, 3.0), ("3.0x+", 3.0, 999)]:
        mask = (results_df["vol_ratio"] >= lo) & (results_df["vol_ratio"] < hi)
        sub_er = results_df.loc[mask, "excess_ret_20d"].dropna()
        sub_results[f"vol_{label}_n"] = len(sub_er)
        sub_results[f"vol_{label}_mean_pct"] = round(float(sub_er.mean()) * 100, 4) if len(sub_er) > 0 else 0.0
        sub_results[f"vol_{label}_winrate"] = round(float((sub_er > 0).mean()) * 100, 2) if len(sub_er) > 0 else 0.0

    statistics.update(sub_results)

    # ============================================================
    # バックテスト
    # ============================================================
    COST_RATE = 0.001 + 0.0005  # 片道手数料0.1% + スリッページ0.05%
    INITIAL_CAPITAL = 10_000_000.0

    bt_df = results_df.dropna(subset=["ret_20d", "entry_date"]).copy()
    bt_df["entry_date"] = pd.to_datetime(bt_df["entry_date"])
    if "exit_date" in bt_df.columns:
        bt_df["exit_date"] = pd.to_datetime(bt_df["exit_date"])

    # 日次エクイティカーブの構築
    all_dates = sorted(topix_df[topix_df["date"] >= pd.Timestamp(START_DATE)]["date"].unique())
    equity = INITIAL_CAPITAL
    equity_curve = []
    positions = []  # (entry_date, exit_idx, code, entry_price, allocated_capital)
    trade_log = []

    # エントリー日ごとにグループ化
    entry_groups = {}
    for _, row in bt_df.iterrows():
        ed = pd.Timestamp(row["entry_date"])
        if ed not in entry_groups:
            entry_groups[ed] = []
        entry_groups[ed].append(row)

    cash = INITIAL_CAPITAL
    active_positions = []

    for d in all_dates:
        d_ts = pd.Timestamp(d)

        # ポジションのクローズ（20営業日経過）
        still_active = []
        for pos in active_positions:
            if d_ts >= pos["exit_date"]:
                # 終了: exit_priceで手仕舞い
                exit_val = pos["shares"] * pos["exit_price"] * (1 - COST_RATE)
                cash += exit_val
                trade_log.append({
                    "date": str(pos["exit_date"])[:10],
                    "code": pos["code"],
                    "action": "sell",
                    "shares": pos["shares"],
                    "price": round(pos["exit_price"], 1),
                })
            else:
                still_active.append(pos)
        active_positions = still_active

        # 新規エントリー
        if d_ts in entry_groups:
            entries = entry_groups[d_ts]
            n_entries = len(entries)
            alloc_per_stock = min(cash * 0.9 / max(n_entries, 1), cash * 0.3)
            for row in entries:
                if cash < alloc_per_stock or alloc_per_stock <= 0:
                    break
                ep = row["entry_price"]
                if ep <= 0:
                    continue
                shares = max(int(alloc_per_stock / ep), 1)
                cost = shares * ep * (1 + COST_RATE)
                if cost > cash:
                    shares = max(int(cash / (ep * (1 + COST_RATE))), 1)
                    cost = shares * ep * (1 + COST_RATE)
                if cost > cash:
                    continue
                cash -= cost

                exit_price = row.get("exit_price", ep)
                exit_date = row.get("exit_date", d_ts + pd.Timedelta(days=30))

                active_positions.append({
                    "code": row["code"],
                    "shares": shares,
                    "entry_price": ep,
                    "exit_price": exit_price,
                    "exit_date": pd.Timestamp(exit_date),
                })
                trade_log.append({
                    "date": str(d_ts)[:10],
                    "code": row["code"],
                    "action": "buy",
                    "shares": shares,
                    "price": round(ep, 1),
                })

        # 日次の評価額（簡易: 保有ポジションは最終exit_priceで線形補間せず、entry_priceベース）
        pos_value = sum(p["shares"] * p["entry_price"] for p in active_positions)
        total_value = cash + pos_value
        equity_curve.append({"date": str(d)[:10], "value": round(total_value, 0)})

    # ベンチマークカーブ
    topix_analysis = topix_df[topix_df["date"] >= pd.Timestamp(START_DATE)].copy()
    if len(topix_analysis) > 0:
        topix_base = topix_analysis["topix_close"].iloc[0]
        benchmark_curve = [
            {
                "date": str(row["date"])[:10],
                "value": round(INITIAL_CAPITAL * row["topix_close"] / topix_base, 0),
            }
            for _, row in topix_analysis.iterrows()
        ]
        bm_final = topix_analysis["topix_close"].iloc[-1] / topix_base - 1.0
    else:
        benchmark_curve = []
        bm_final = 0.0

    # バックテスト指標の計算
    if len(equity_curve) >= 2:
        eq_values = [e["value"] for e in equity_curve]
        cum_ret = eq_values[-1] / eq_values[0] - 1.0
        n_years = len(eq_values) / 252.0
        annual_ret = (1 + cum_ret) ** (1 / max(n_years, 0.01)) - 1.0
        daily_rets = np.diff(eq_values) / np.array(eq_values[:-1])
        sharpe = float(np.mean(daily_rets) / np.std(daily_rets) * np.sqrt(252)) if np.std(daily_rets) > 0 else 0.0
        running_max = np.maximum.accumulate(eq_values)
        drawdowns = (np.array(eq_values) - running_max) / running_max
        max_dd = float(np.min(drawdowns))
    else:
        cum_ret = 0.0
        annual_ret = 0.0
        sharpe = 0.0
        max_dd = 0.0

    # ベンチマーク指標
    if len(topix_analysis) >= 2:
        bm_values = [b["value"] for b in benchmark_curve]
        bm_daily = np.diff(bm_values) / np.array(bm_values[:-1])
        bm_annual = (1 + bm_final) ** (1 / max(len(bm_values) / 252.0, 0.01)) - 1.0
        bm_sharpe = float(np.mean(bm_daily) / np.std(bm_daily) * np.sqrt(252)) if np.std(bm_daily) > 0 else 0.0
    else:
        bm_annual = 0.0
        bm_sharpe = 0.0

    total_trades = len(bt_df)
    bt_wins = (bt_df["excess_ret_20d"] > 0).sum() if len(bt_df) > 0 else 0
    bt_win_rate = float(bt_wins / total_trades * 100) if total_trades > 0 else 0.0

    backtest = {
        "cumulative_return": round(cum_ret * 100, 2),
        "annual_return": round(annual_ret * 100, 2),
        "sharpe_ratio": round(sharpe, 4),
        "max_drawdown": round(max_dd * 100, 2),
        "win_rate": round(bt_win_rate, 2),
        "total_trades": total_trades,
        "benchmark_cumulative_return": round(bm_final * 100, 2),
        "benchmark_annual_return": round(bm_annual * 100, 2),
        "benchmark_sharpe_ratio": round(bm_sharpe, 4),
        "equity_curve": equity_curve,
        "benchmark_curve": benchmark_curve,
        "trade_log": trade_log[-200:],  # 最大200件
    }

    # ============================================================
    # 直近シグナル例
    # ============================================================
    stock_name_map = dict(zip(stocks_df["code"], stocks_df["name"]))
    recent_df = results_df.sort_values("signal_date", ascending=False).head(10)
    recent_examples = []
    for _, row in recent_df.iterrows():
        name = stock_name_map.get(row["code"], row["code"])
        ret_val = row.get("ret_20d", row.get("excess_ret_20d", np.nan))
        ret_pct = round(float(ret_val) * 100, 2) if not np.isnan(ret_val) else 0.0
        recent_examples.append({
            "date": str(row["signal_date"])[:10],
            "description": f"{name}({row['code']}): 連続陽線{int(row['consec_days'])}日, 出来高倍率{row['vol_ratio']:.1f}倍",
            "return_pct": ret_pct,
        })

    metadata = {
        "universe_codes": universe_codes,
        "data_period": f"{START_DATE} ~ {END_DATE}",
        "description": (
            "陽線が3日以上連続し、かつ出来高が直近20日平均の1.5倍以上に増加した銘柄の"
            "シグナル発生後20営業日間の超過リターン（対TOPIX）を検証するイベントスタディ分析。"
            f"サンプル銘柄数: {len(universe_codes)}, シグナル検出数: {n_condition}"
        ),
    }

    return {
        "statistics": statistics,
        "backtest": backtest,
        "recent_examples": recent_examples,
        "metadata": metadata,
    }


def _calc_profit_factor(excess_returns):
    if len(excess_returns) == 0:
        return 0.0
    gains = excess_returns[excess_returns > 0].sum()
    losses = abs(excess_returns[excess_returns < 0].sum())
    if losses == 0:
        return float("inf") if gains > 0 else 0.0
    return round(float(gains / losses), 4)


def _empty_result(universe_codes, start, end):
    return {
        "statistics": {
            "test_name": "連続陽線・出来高急増シグナルの超過リターン検証",
            "condition_mean": 0.0, "baseline_mean": 0.0,
            "condition_std": 0.0, "baseline_std": 0.0,
            "t_statistic": 0.0, "p_value": 1.0, "cohens_d": 0.0,
            "win_rate_condition": 0.0, "win_rate_baseline": 50.0,
            "n_condition": 0, "n_baseline": 0, "is_significant": False,
        },
        "backtest": {
            "cumulative_return": 0.0, "annual_return": 0.0,
            "sharpe_ratio": 0.0, "max_drawdown": 0.0,
            "win_rate": 0.0, "total_trades": 0,
            "benchmark_cumulative_return": 0.0, "benchmark_annual_return": 0.0,
            "benchmark_sharpe_ratio": 0.0,
            "equity_curve": [], "benchmark_curve": [], "trade_log": [],
        },
        "recent_examples": [],
        "metadata": {
            "universe_codes": universe_codes,
            "data_period": f"{start} ~ {end}",
            "description": "シグナルが検出されませんでした。",
        },
    }
```

---

_Knowledge ID: 3 | Run ID: 15 | 生成日: 2026-02-23 11:12:37_