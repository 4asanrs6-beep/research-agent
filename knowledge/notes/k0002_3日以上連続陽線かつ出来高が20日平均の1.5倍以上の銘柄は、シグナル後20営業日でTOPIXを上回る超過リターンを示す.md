# 3日以上連続陽線かつ出来高が20日平均の1.5倍以上の銘柄は、シグナル後20営業日でTOPIXを上回る超過リターンを示す。

- **Knowledge ID**: 2
- **Run ID**: 14
- **判定**: needs_review
- **作成日**: 2026-02-23 10:54:20
- **タグ**: 連続陽線, 出来高急増, モメンタム, テクニカル分析, イベントスタディ, 日本株, TOPIX, 実行エラー, 再検証必要

## 分析条件

- **対象**: 東証上場全銘柄のうち、シグナル判定日時点で時価総額50億円以上の銘柄
- **期間**: 2025-01-01 ~ 2026-02-23
- **アプローチ**: イベントスタディ方式により、シグナル発生をイベントとして捉え、イベント後20営業日の累積超過リターン（対TOPIX）を算出し、統計的有意性を検証する。加えて、サバイバーシップバイアスの軽減、銘柄・時期の偏りの確認、およびロバスト性チェックを行う。
  1. ステップ1: データ取得と前処理 — J-Quants APIから対象期間の全銘柄日足データ（OHLCV・調整後価格）、TOPIX日次四本値、上場銘柄一覧（時価総額含む）を取得。株式分割・併合等の調整済み価格を使用し、上場廃止銘柄も含めてサバイバーシップバイアスを排除する。
  2. ステップ2: ユニバース構築 — 各営業日において時価総額50億円以上の銘柄をフィルタリングし、動的ユニバースを構築する。
  3. ステップ3: シグナル判定 — 各銘柄・各営業日について以下の2条件を同時に満たすかを判定する。(a) 当日を含む直近3営業日以上連続で陽線（調整後終値＞調整後始値）であること。(b) 当日の出来高が直近20営業日の平均出来高の1.5倍以上であること。条件を満たした最初の日をシグナル発生日とし、同一銘柄で連続してシグナルが発生した場合は最初のシグナルから20営業日以内の重複シグナルを除外する（独立イベントの確保）。
  4. ステップ4: リターン計測 — シグナル発生日の翌営業日始値で購入したと仮定し、20営業日後の終値までの保有リターンを算出する。同期間のTOPIXリターンも算出し、超過リターン（銘柄リターン − TOPIXリターン）を計算する。
  5. ステップ5: 記述統計量の算出 — シグナル発生件数、超過リターンの平均・中央値・標準偏差・最大値・最小値・勝率（超過リターン＞0の割合）を算出する。月別・セクター別のシグナル発生分布も確認する。
  6. ステップ6: 統計的有意性の検定 — 超過リターンの平均がゼロと有意に異なるかをt検定で検証する。分布の正規性が棄却される場合はWilcoxonの符号順位検定も実施する。多重検定の問題を考慮し、Bonferroni補正またはFDR補正を適用する。
  7. ステップ7: ロバスト性チェック — (a) 連続陽線日数の閾値を2日・4日・5日に変更、(b) 出来高倍率の閾値を1.2倍・2.0倍・2.5倍に変更、(c) 保有期間を5日・10日・40日に変更した場合のパラメータ感応度分析を行い、結果の頑健性を確認する。
  8. ステップ8: サブグループ分析 — 時価総額階層別（50-300億円、300-1000億円、1000億円以上）およびセクター別に超過リターンを分解し、特定の属性に結果が偏っていないかを確認する。
  9. ステップ9: 結果の可視化と総括 — シグナル後の累積超過リターン推移（イベントウィンドウ: -5日〜+30日）のグラフ、パラメータ感応度ヒートマップ、月次シグナル発生件数の時系列チャートを作成し、投資戦略としての有効性を総合的に判断する。

## バックテスト結果

| 指標 | 値 |
|------|-----|
| 累計リターン | 0.00% |
| 年率リターン | 0.00% |
| シャープ比 | 0.00 |
| 最大DD | 0.00% |
| 勝率 | 0.0% |
| 取引回数 | 0 |
| BM累計リターン | 0.00% |

## 解釈

統計分析およびバックテストの全結果がゼロ値であり、エラーメッセージ「'__import__'」が記録されていることから、分析パイプラインが正常に実行されていないと判断される。仮説自体の妥当性を評価するに足るデータが得られておらず、技術的問題の解消後に再実行が必要である。現時点では仮説の有効・無効いずれの判定も不可能である。

**判断理由:**
- n_condition=0, n_baseline=0であり、シグナル検出およびベースライン構築が一切行われていない。サンプルサイズゼロでは統計的検証が成立しない。
- エラーフィールドに「'__import__'」が記録されており、Pythonの実行環境においてセキュリティ制約またはサンドボックス制限により__import__関数の使用がブロックされた可能性が高い。これはデータ取得・前処理段階での致命的な実行失敗を示唆する。
- p値=1.0, t統計量=0.0, Cohen's d=0.0, 勝率=0.0は全て初期値のままであり、計算が一切実行されていないことを裏付ける。
- バックテスト結果もtotal_trades=0, equity_curve=[], trade_log=[]と完全に空であり、シミュレーションが開始すらされていない。

**強み:**
- 分析計画自体は方法論的に極めて堅実である。イベントスタディ方式の採用、サバイバーシップバイアスへの配慮、重複シグナル除外ロジック、パラメータ感応度分析、サブグループ分析など、定量分析のベストプラクティスを網羅している。
- 翌営業日始値での購入仮定はルックアヘッドバイアスを回避する適切な設計である。
- 時価総額50億円以上のフィルタリングにより、流動性の極端に低い超小型株を除外する設計は実運用上も妥当である。
- ロバスト性チェックとして連続陽線日数・出来高倍率・保有期間の各パラメータを複数水準で検証する計画は、データマイニングバイアスへの対策として適切である。

**弱み:**
- 実行環境の制約により分析が完了しておらず、仮説に対する実証的エビデンスが皆無である。
- エラーハンドリングが不十分であり、__import__制限に遭遇した時点で全処理がゼロ値にフォールバックしており、部分的な結果すら取得できていない。
- 分析計画においてトランザクションコスト（売買手数料・スリッページ・マーケットインパクト）の考慮が明示されていない。出来高急増銘柄はスプレッド拡大やスリッページが大きくなる傾向があり、実運用リターンを過大評価するリスクがある。
- 連続陽線＋出来高急増というシグナルは市場全体のモメンタム局面（例：2024年初の日経急騰期）に集中発生しやすく、独立イベントの仮定が崩れる可能性がある。クラスタリング補正（例：Newey-West標準誤差やクラスター頑健標準誤差）が計画に含まれていない。

**改善提案:**
- 実行環境の__import__制限を解消するか、制限のない環境（ローカルPython環境やGoogle Colab等）で再実行する。必要なライブラリ（pandas, scipy, numpy等）は事前にインポートしておく。
- J-Quants APIのデータ取得部分を分離し、CSVファイルとして事前にダウンロードしておくことで、分析コードの実行時にAPI依存を排除する。
- トランザクションコストとして片道10-30bpsのスリッページを仮定したネットリターンでの検証を追加する。
- シグナル発生の時間的クラスタリングに対処するため、カレンダータイム・ポートフォリオ方式（月次リバランス）での検証も並行して実施する。
- Fama-French 3ファクターまたは5ファクターモデルによるリスク調整済みリターン（アルファ）の算出を追加し、シグナルの超過リターンがサイズ・バリュー・モメンタム等の既知ファクターで説明されないかを確認する。
- ランダムにシグナル発生日を生成するプラセボテスト（permutation test）を1000回程度実施し、実際のシグナルの超過リターンがランダム対比で有意かを確認する。

## 結論

**この仮説は追加検証が必要です。**

## 分析コード

```python
import pandas as pd
import numpy as np
from scipy import stats


def run_analysis(data_provider):
    # ===== Parameters =====
    CONSEC_BULLISH = 3
    VOL_RATIO_THRESH = 1.5
    VOL_MA_WINDOW = 20
    HOLDING_PERIOD = 20
    START_DATE = "2025-01-01"
    END_DATE = "2026-02-23"
    WARMUP_START = "2024-11-01"
    MAX_STOCKS = 80
    TX_COST_ONEWAY = 0.001  # 10bps

    def empty_result(codes, desc="シグナルが検出されませんでした。"):
        return {
            "statistics": {
                "test_name": "連続陽線・出来高急増シグナル 超過リターン検証（イベントスタディ）",
                "condition_mean": 0.0, "baseline_mean": 0.0,
                "condition_std": 0.0, "baseline_std": 0.0,
                "t_statistic": 0.0, "p_value": 1.0, "cohens_d": 0.0,
                "win_rate_condition": 0.0, "win_rate_baseline": 0.0,
                "n_condition": 0, "n_baseline": 0, "is_significant": False,
            },
            "backtest": {
                "cumulative_return": 0.0, "annual_return": 0.0,
                "sharpe_ratio": 0.0, "max_drawdown": 0.0,
                "win_rate": 0.0, "total_trades": 0,
                "benchmark_cumulative_return": 0.0, "benchmark_annual_return": 0.0,
                "benchmark_sharpe_ratio": 0.0,
                "equity_curve": [], "benchmark_curve": [], "trade_log": [],
            },
            "recent_examples": [],
            "metadata": {
                "universe_codes": [str(c) for c in codes],
                "data_period": f"{START_DATE} ~ {END_DATE}",
                "description": desc,
            },
        }

    try:
        # ============================================================
        # Step 1: Universe Construction
        # ============================================================
        stocks_df = data_provider.get_listed_stocks()
        stocks_df["scale_category"] = stocks_df["scale_category"].fillna("")

        valid_scales = [
            "TOPIX Core30", "TOPIX Large70", "TOPIX Mid400",
            "TOPIX Small 1", "TOPIX Small 2",
        ]
        universe_df = stocks_df[stocks_df["scale_category"].isin(valid_scales)].copy()

        if len(universe_df) == 0:
            return empty_result([], "時価総額フィルタに合致する銘柄がありません。")

        if len(universe_df) > MAX_STOCKS:
            universe_df = universe_df.sample(n=MAX_STOCKS, random_state=42)

        universe_codes = universe_df["code"].tolist()

        code_info = {}
        for _, row in stocks_df.iterrows():
            code_info[row["code"]] = {
                "name": str(row.get("name", "")),
                "sector": str(row.get("sector_33_name", "")),
                "scale": str(row.get("scale_category", "")),
            }

        # ============================================================
        # Step 2: TOPIX Index Data
        # ============================================================
        topix_df = data_provider.get_index_prices(
            index_code="0000", start_date=WARMUP_START, end_date=END_DATE
        )
        topix_df["date"] = pd.to_datetime(topix_df["date"])
        topix_df = topix_df.sort_values("date").reset_index(drop=True)
        topix_close_map = dict(zip(topix_df["date"], topix_df["close"]))
        topix_dates_sorted = sorted(topix_close_map.keys())

        # ============================================================
        # Step 3: Price Data Retrieval
        # ============================================================
        all_stock_data = {}
        for code in universe_codes:
            try:
                df = data_provider.get_price_daily(
                    code=code, start_date=WARMUP_START, end_date=END_DATE
                )
                if df is not None and len(df) >= VOL_MA_WINDOW + CONSEC_BULLISH:
                    df["date"] = pd.to_datetime(df["date"])
                    df = df.sort_values("date").reset_index(drop=True)
                    df = df.dropna(subset=["adj_open", "adj_close", "adj_volume"])
                    df = df[
                        (df["adj_open"] > 0)
                        & (df["adj_close"] > 0)
                        & (df["adj_volume"] > 0)
                    ].reset_index(drop=True)
                    if len(df) >= VOL_MA_WINDOW + CONSEC_BULLISH:
                        all_stock_data[code] = df
            except Exception:
                continue

        if not all_stock_data:
            return empty_result(universe_codes, "株価データを取得できませんでした。")

        # Helper: find nearest TOPIX close for a given date
        def _topix_close_nearest(d):
            val = topix_close_map.get(d)
            if val is not None:
                return val
            candidates = [td for td in topix_dates_sorted if td <= d]
            return topix_close_map[candidates[-1]] if candidates else None

        # ============================================================
        # Step 4: Signal Detection
        # ============================================================
        signals = []
        analysis_start_ts = pd.Timestamp(START_DATE)

        for code, df in all_stock_data.items():
            n = len(df)
            adj_open = df["adj_open"].values
            adj_close = df["adj_close"].values
            adj_vol = df["adj_volume"].values.astype(float)
            dates_arr = df["date"].values

            # Consecutive bullish candle count
            bullish = (adj_close > adj_open).astype(np.int32)
            consec = np.zeros(n, dtype=np.int32)
            consec[0] = bullish[0]
            for i in range(1, n):
                consec[i] = (consec[i - 1] + 1) * bullish[i]

            # 20-day volume moving average and ratio
            vol_ma = (
                pd.Series(adj_vol)
                .rolling(window=VOL_MA_WINDOW, min_periods=VOL_MA_WINDOW)
                .mean()
                .values
            )
            with np.errstate(divide="ignore", invalid="ignore"):
                vol_ratio = np.where(
                    (vol_ma > 0) & ~np.isnan(vol_ma), adj_vol / vol_ma, 0.0
                )

            # Signal mask
            signal_mask = (
                (dates_arr >= np.datetime64(START_DATE))
                & (consec >= CONSEC_BULLISH)
                & (vol_ratio >= VOL_RATIO_THRESH)
                & (~np.isnan(vol_ma))
                & (vol_ma > 0)
            )
            candidate_idx = np.where(signal_mask)[0]

            last_sig_idx = -(HOLDING_PERIOD + 100)
            for idx in candidate_idx:
                # Skip overlapping signals
                if idx - last_sig_idx <= HOLDING_PERIOD:
                    continue
                # Ensure full holding period data exists
                exit_idx = idx + 1 + HOLDING_PERIOD
                if exit_idx >= n or (idx + 1) >= n:
                    continue

                entry_price = adj_open[idx + 1]
                exit_price = adj_close[exit_idx]
                if entry_price <= 0 or exit_price <= 0:
                    continue

                signal_date = pd.Timestamp(dates_arr[idx])
                entry_date = pd.Timestamp(dates_arr[idx + 1])
                exit_date = pd.Timestamp(dates_arr[exit_idx])

                stock_ret = exit_price / entry_price - 1.0

                # TOPIX return over the same period (signal close → exit close)
                topix_sig = _topix_close_nearest(signal_date)
                topix_exit = _topix_close_nearest(exit_date)
                if topix_sig is None or topix_exit is None or topix_sig <= 0:
                    continue
                topix_ret = topix_exit / topix_sig - 1.0
                excess_ret = stock_ret - topix_ret

                # Daily returns for equity curve
                closes_hold = adj_close[idx + 1 : exit_idx + 1]
                dates_hold = dates_arr[idx + 1 : exit_idx + 1]
                daily_ret = np.empty(len(closes_hold))
                daily_ret[0] = closes_hold[0] / entry_price - 1.0
                if len(closes_hold) > 1:
                    daily_ret[1:] = closes_hold[1:] / closes_hold[:-1] - 1.0

                info = code_info.get(code, {"name": "", "sector": "", "scale": ""})
                signals.append(
                    {
                        "signal_date": signal_date,
                        "entry_date": entry_date,
                        "exit_date": exit_date,
                        "code": str(code),
                        "name": info["name"],
                        "sector": info["sector"],
                        "scale": info["scale"],
                        "entry_price": float(entry_price),
                        "exit_price": float(exit_price),
                        "stock_return": float(stock_ret),
                        "topix_return": float(topix_ret),
                        "excess_return": float(excess_ret),
                        "consec_days": int(consec[idx]),
                        "vol_ratio": float(vol_ratio[idx]),
                        "daily_dates": dates_hold,
                        "daily_returns": daily_ret,
                    }
                )
                last_sig_idx = idx

        if not signals:
            return empty_result(
                universe_codes,
                f"分析対象{len(all_stock_data)}銘柄からシグナルが検出されませんでした。",
            )

        n_signals = len(signals)
        excess_returns = np.array([s["excess_return"] for s in signals])

        # ============================================================
        # Step 5: Statistical Tests
        # ============================================================
        mean_er = float(np.mean(excess_returns))
        median_er = float(np.median(excess_returns))
        std_er = float(np.std(excess_returns, ddof=1)) if n_signals > 1 else 0.0

        # One-sided t-test (H1: mean > 0)
        if n_signals >= 2 and std_er > 0:
            t_stat, p_two = stats.ttest_1samp(excess_returns, 0.0)
            p_value = float(p_two / 2.0) if t_stat > 0 else float(1.0 - p_two / 2.0)
            t_stat = float(t_stat)
        else:
            t_stat, p_value = 0.0, 1.0

        # Wilcoxon signed-rank test (one-sided)
        wilcoxon_p = None
        if n_signals >= 6:
            try:
                w_stat, w_p_two = stats.wilcoxon(excess_returns)
                wilcoxon_p = float(w_p_two / 2.0) if np.median(excess_returns) > 0 else float(1.0 - w_p_two / 2.0)
            except Exception:
                pass

        # Shapiro-Wilk normality test
        shapiro_p = None
        if 3 <= n_signals <= 5000:
            try:
                _, sw_p = stats.shapiro(excess_returns)
                shapiro_p = float(sw_p)
            except Exception:
                pass

        # Bootstrap 95% confidence interval (10,000 iterations)
        np.random.seed(42)
        boot_means = np.array(
            [
                np.mean(np.random.choice(excess_returns, size=n_signals, replace=True))
                for _ in range(10000)
            ]
        )
        ci_lower = float(np.percentile(boot_means, 2.5))
        ci_upper = float(np.percentile(boot_means, 97.5))

        # Effect size (Cohen's d)
        cohens_d = mean_er / std_er if std_er > 0 else 0.0

        # Win rate
        win_rate = float(np.mean(excess_returns > 0))

        # Sharpe ratio (annualized, based on holding period)
        periods_per_year = 252.0 / HOLDING_PERIOD
        sharpe_excess = (
            (mean_er * periods_per_year) / (std_er * np.sqrt(periods_per_year))
            if std_er > 0
            else 0.0
        )

        # Profit factor
        gains = excess_returns[excess_returns > 0]
        losses = excess_returns[excess_returns < 0]
        if len(losses) > 0 and np.sum(losses) != 0:
            profit_factor = float(np.sum(gains) / abs(np.sum(losses)))
        elif len(gains) > 0:
            profit_factor = None  # infinite
        else:
            profit_factor = 0.0

        # Transaction-cost adjusted returns (round-trip 20bps)
        cost_adj_returns = excess_returns - 2.0 * TX_COST_ONEWAY
        cost_adj_mean = float(np.mean(cost_adj_returns))
        if n_signals >= 2 and np.std(cost_adj_returns, ddof=1) > 0:
            ca_t, ca_p2 = stats.ttest_1samp(cost_adj_returns, 0.0)
            cost_adj_p = float(ca_p2 / 2.0) if ca_t > 0 else float(1.0 - ca_p2 / 2.0)
        else:
            cost_adj_p = 1.0

        # ============================================================
        # Step 6: Baseline (Random Entry for Comparison)
        # ============================================================
        np.random.seed(123)
        baseline_returns = []
        for code, df in all_stock_data.items():
            adf = df[df["date"] >= pd.Timestamp(START_DATE)].reset_index(drop=True)
            max_start = len(adf) - HOLDING_PERIOD - 1
            if max_start <= 0:
                continue
            n_rand = min(3, max_start)
            rand_idx = np.random.choice(max_start, size=n_rand, replace=False)
            for ri in rand_idx:
                ep = adf.iloc[ri]["adj_open"]
                xp = adf.iloc[ri + HOLDING_PERIOD]["adj_close"]
                if ep <= 0:
                    continue
                ed = pd.Timestamp(adf.iloc[ri]["date"])
                xd = pd.Timestamp(adf.iloc[ri + HOLDING_PERIOD]["date"])
                s_ret = xp / ep - 1.0
                t_e = _topix_close_nearest(ed)
                t_x = _topix_close_nearest(xd)
                if t_e is None or t_x is None or t_e <= 0:
                    continue
                baseline_returns.append(s_ret - (t_x / t_e - 1.0))

        baseline_arr = np.array(baseline_returns) if baseline_returns else np.array([0.0])

        # ============================================================
        # Step 7: Backtest Equity Curve
        # ============================================================
        daily_pos_returns = {}
        for sig in signals:
            for j in range(len(sig["daily_dates"])):
                d = pd.Timestamp(sig["daily_dates"][j])
                daily_pos_returns.setdefault(d, []).append(sig["daily_returns"][j])

        analysis_topix_dates = [
            d
            for d in topix_dates_sorted
            if pd.Timestamp(START_DATE) <= d <= pd.Timestamp(END_DATE)
        ]

        if not analysis_topix_dates:
            return empty_result(universe_codes, "分析期間のTOPIXデータがありません。")

        port_daily_list = []
        bench_daily_list = []
        prev_tc = None

        for d in analysis_topix_dates:
            # Portfolio return: equal-weight average of active positions; 0 if idle
            pos = daily_pos_returns.get(d)
            port_daily_list.append(float(np.mean(pos)) if pos else 0.0)

            # TOPIX daily return
            tc = topix_close_map.get(d, prev_tc)
            if prev_tc is not None and prev_tc > 0 and tc is not None:
                bench_daily_list.append(tc / prev_tc - 1.0)
            else:
                bench_daily_list.append(0.0)
            if tc is not None:
                prev_tc = tc

        port_daily = np.array(port_daily_list)
        bench_daily = np.array(bench_daily_list)
        port_cum = np.cumprod(1.0 + port_daily)
        bench_cum = np.cumprod(1.0 + bench_daily)

        total_return = float(port_cum[-1] - 1.0)
        bench_total_return = float(bench_cum[-1] - 1.0)

        n_days = len(analysis_topix_dates)
        ann_factor = 252.0 / n_days if n_days > 0 else 1.0
        annual_return = float((1.0 + total_return) ** ann_factor - 1.0)
        bench_annual_return = float((1.0 + bench_total_return) ** ann_factor - 1.0)

        port_sharpe = (
            float(np.mean(port_daily) / np.std(port_daily, ddof=1) * np.sqrt(252))
            if np.std(port_daily, ddof=1) > 0
            else 0.0
        )
        bench_sharpe = (
            float(np.mean(bench_daily) / np.std(bench_daily, ddof=1) * np.sqrt(252))
            if np.std(bench_daily, ddof=1) > 0
            else 0.0
        )

        running_max = np.maximum.accumulate(port_cum)
        max_dd = float(np.min((port_cum - running_max) / running_max))

        equity_curve = [
            {
                "date": analysis_topix_dates[i].strftime("%Y-%m-%d"),
                "value": round(float(port_cum[i]), 6),
            }
            for i in range(n_days)
        ]
        benchmark_curve = [
            {
                "date": analysis_topix_dates[i].strftime("%Y-%m-%d"),
                "value": round(float(bench_cum[i]), 6),
            }
            for i in range(n_days)
        ]

        # Trade log
        trade_log = []
        for sig in signals:
            trade_log.append(
                {
                    "date": sig["entry_date"].strftime("%Y-%m-%d"),
                    "code": sig["code"],
                    "action": "BUY",
                    "shares": 100,
                    "price": round(sig["entry_price"], 1),
                }
            )
            trade_log.append(
                {
                    "date": sig["exit_date"].strftime("%Y-%m-%d"),
                    "code": sig["code"],
                    "action": "SELL",
                    "shares": 100,
                    "price": round(sig["exit_price"], 1),
                }
            )
        trade_log.sort(key=lambda x: x["date"])

        # ============================================================
        # Step 8: Recent Examples (top 10 by date descending)
        # ============================================================
        signals_sorted = sorted(signals, key=lambda x: x["signal_date"], reverse=True)
        recent_examples = []
        for sig in signals_sorted[:10]:
            recent_examples.append(
                {
                    "date": sig["signal_date"].strftime("%Y-%m-%d"),
                    "description": (
                        f"{sig['code']} {sig['name']} | "
                        f"連続陽線{sig['consec_days']}日 | "
                        f"出来高倍率{sig['vol_ratio']:.1f}x | "
                        f"銘柄リターン{sig['stock_return']*100:+.2f}% | "
                        f"超過リターン{sig['excess_return']*100:+.2f}%"
                    ),
                    "return_pct": round(sig["excess_return"] * 100, 2),
                }
            )

        # ============================================================
        # Step 9: Assemble Result
        # ============================================================
        result = {
            "statistics": {
                "test_name": "連続陽線・出来高急増シグナル 超過リターン検証（イベントスタディ）",
                "condition_mean": round(mean_er, 6),
                "baseline_mean": round(float(np.mean(baseline_arr)), 6),
                "condition_std": round(std_er, 6),
                "baseline_std": round(float(np.std(baseline_arr, ddof=1)) if len(baseline_arr) > 1 else 0.0, 6),
                "t_statistic": round(t_stat, 4),
                "p_value": round(p_value, 6),
                "cohens_d": round(float(cohens_d), 4),
                "win_rate_condition": round(win_rate, 4),
                "win_rate_baseline": round(float(np.mean(baseline_arr > 0)), 4),
                "n_condition": n_signals,
                "n_baseline": len(baseline_arr),
                "is_significant": bool(p_value < 0.05),
                # --- Extended statistics ---
                "median_excess_return": round(median_er, 6),
                "max_excess_return": round(float(np.max(excess_returns)), 6),
                "min_excess_return": round(float(np.min(excess_returns)), 6),
                "wilcoxon_p_value": round(wilcoxon_p, 6) if wilcoxon_p is not None else None,
                "shapiro_wilk_p_value": round(shapiro_p, 6) if shapiro_p is not None else None,
                "bootstrap_ci_95_lower": round(ci_lower, 6),
                "bootstrap_ci_95_upper": round(ci_upper, 6),
                "sharpe_ratio_excess": round(float(sharpe_excess), 4),
                "profit_factor": round(profit_factor, 4) if profit_factor is not None else None,
                "cost_adjusted_mean_return": round(cost_adj_mean, 6),
                "cost_adjusted_p_value": round(float(cost_adj_p), 6),
                "anomaly_confirmed": bool(
                    p_value < 0.05
                    and mean_er > 0
                    and ci_lower > 0
                ),
            },
            "backtest": {
                "cumulative_return": round(total_return, 6),
                "annual_return": round(annual_return, 6),
                "sharpe_ratio": round(port_sharpe, 4),
                "max_drawdown": round(max_dd, 6),
                "win_rate": round(win_rate, 4),
                "total_trades": n_signals,
                "benchmark_cumulative_return": round(bench_total_return, 6),
                "benchmark_annual_return": round(bench_annual_return, 6),
                "benchmark_sharpe_ratio": round(bench_sharpe, 4),
                "equity_curve": equity_curve,
                "benchmark_curve": benchmark_curve,
                "trade_log": trade_log,
            },
            "recent_examples": recent_examples,
            "metadata": {
                "universe_codes": [str(c) for c in universe_codes],
                "data_period": f"{START_DATE} ~ {END_DATE}",
                "description": (
                    f"連続陽線({CONSEC_BULLISH}日以上)＋出来高急増"
                    f"(20日平均の{VOL_RATIO_THRESH}倍以上)シグナルの"
                    f"超過リターン(対TOPIX)検証。"
                    f"TOPIX構成銘柄からサンプリングした{len(all_stock_data)}銘柄を分析。"
                    f"シグナル検出数: {n_signals}件。"
                    f"保有期間: {HOLDING_PERIOD}営業日。"
                    f"片側t検定 p={p_value:.4f}, "
                    f"平均超過リターン={mean_er*100:.2f}%, "
                    f"勝率={win_rate*100:.1f}%。"
                ),
            },
        }

        return result

    except Exception as e:
        return {
            "statistics": {
                "test_name": "連続陽線・出来高急増シグナル 超過リターン検証",
                "condition_mean": 0.0, "baseline_mean": 0.0,
                "condition_std": 0.0, "baseline_std": 0.0,
                "t_statistic": 0.0, "p_value": 1.0, "cohens_d": 0.0,
                "win_rate_condition": 0.0, "win_rate_baseline": 0.0,
                "n_condition": 0, "n_baseline": 0,
                "is_significant": False,
                "error": str(e),
            },
            "backtest": {
                "cumulative_return": 0.0, "annual_return": 0.0,
                "sharpe_ratio": 0.0, "max_drawdown": 0.0,
                "win_rate": 0.0, "total_trades": 0,
                "benchmark_cumulative_return": 0.0, "benchmark_annual_return": 0.0,
                "benchmark_sharpe_ratio": 0.0,
                "equity_curve": [], "benchmark_curve": [], "trade_log": [],
            },
            "recent_examples": [],
            "metadata": {
                "universe_codes": [],
                "data_period": f"2025-01-01 ~ 2026-02-23",
                "description": f"エラーが発生しました: {str(e)}",
            },
        }
```

---

_Knowledge ID: 2 | Run ID: 14 | 生成日: 2026-02-23 10:54:20_